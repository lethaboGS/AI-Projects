{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba100652-ab0d-44a0-a145-00b5a2b06ff4",
   "metadata": {},
   "source": [
    "<h1 align=left><font size = 6> Vision Transformers Using PyTorch </font></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955effd",
   "metadata": {},
   "source": [
    "<h2>Objective</h2>\n",
    "\n",
    "This notebook demonstrates how to use a custom trained PyTorch CNN model to extract feature maps and use them with a Vision Transformer (ViT) architecture to create a CNN-ViT hybrid architecture.\n",
    "\n",
    "Task:\n",
    "<ul>\n",
    "    \n",
    "1. Load the custom trained PyTorch CNN model\n",
    "2. Extract feature maps from the PyTorch model\n",
    "3. Prepare tokens for the Vision Transformer\n",
    "4. Build the Vision Transformer encoder\n",
    "5. Train and evaluate the hybrid model\n",
    "\n",
    "</ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b77975cb-407e-4066-b3c7-4cc5e59ceb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write permissions available for downloading and extracting the dataset tar file\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ae89ad2be1046e9bb397efbead65632",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading images-dataSAT.tar:   0%|          | 0/20243456 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a77a8c392044aa8ad832191352fd83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/6003 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved to '.'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import skillsnetwork\n",
    "\n",
    "data_dir = \".\"\n",
    "dataset_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/4Z1fwRR295-1O3PMQBH6Dg/images-dataSAT.tar\"\n",
    "\n",
    "\n",
    "def check_skillnetwork_extraction(extract_dir):\n",
    "    \"\"\"Check if the environment allows symlink creation for download/extraction.\"\"\"\n",
    "    symlink_test = os.path.join(extract_dir, \"symlink_test\")\n",
    "    if not os.path.exists(symlink_test):\n",
    "        os.symlink(os.path.join(os.sep, \"tmp\"), symlink_test)\n",
    "        print(\"Write permissions available for downloading and extracting the dataset tar file\")\n",
    "        os.unlink(symlink_test)\n",
    "\n",
    "async def download_tar_dataset(url, tar_path, extract_dir):\n",
    "    \"\"\"Download and extract dataset tar file asynchronously.\"\"\"\n",
    "    if not os.path.exists(tar_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(tar_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{tar_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Dataset tar file already exists at: {tar_path}\")\n",
    "    import tarfile\n",
    "    with tarfile.open(tar_path, 'r:*') as tar_ref:\n",
    "        tar_ref.extractall(path=extract_dir)\n",
    "        print(f\"Successfully extracted to '{extract_dir}'.\")\n",
    "\n",
    "try:\n",
    "    check_skillnetwork_extraction(data_dir)\n",
    "    await skillsnetwork.prepare(url=dataset_url, path=data_dir, overwrite=True)\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    print(\"Primary download/extraction method failed.\")\n",
    "    print(\"Falling back to manual download and extraction...\")\n",
    "    import tarfile\n",
    "    import httpx\n",
    "    from pathlib import Path\n",
    "    file_name = Path(dataset_url).name\n",
    "    tar_path = os.path.join(data_dir, file_name)\n",
    "    await download_tar_dataset(dataset_url, tar_path, data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79b410f-de2d-4603-b062-f770fc469d00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 116 ms, sys: 52.5 ms, total: 168 ms\n",
      "Wall time: 33.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%capture captured_output\n",
    "%pip install numpy==1.26 matplotlib==3.9.2 skillsnetwork\n",
    "%pip install torch==2.8.0+cpu torchvision==0.23.0+cpu torchaudio==2.8.0+cpu \\\n",
    "    --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd0fcdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 504 ms, sys: 199 ms, total: 703 ms\n",
      "Wall time: 753 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import time\n",
    "import httpx\n",
    "import random\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "def present_time():\n",
    "        return datetime.now().strftime('%Y%m%d_%H%M%S')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92b7cb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported PyTorch libraries\n",
      "CPU times: user 2.64 s, sys: 488 ms, total: 3.13 s\n",
      "Wall time: 3.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.nn.functional as F\n",
    "print(\"Imported PyTorch libraries\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e211b54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def download_model(url, model_path):\n",
    "    if not os.path.exists(model_path):\n",
    "        try:\n",
    "            print(f\"Downloading from {url}...\")\n",
    "            import httpx\n",
    "            async with httpx.AsyncClient() as client:\n",
    "                response = await client.get(url, follow_redirects=True)\n",
    "                response.raise_for_status()\n",
    "                with open(model_path, \"wb\") as f:\n",
    "                    f.write(response.content)\n",
    "            print(f\"Successfully downloaded '{model_path}'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Download error: {e}\")\n",
    "    else:\n",
    "        print(f\"Model file already downloaded at: {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c8d1c145-57c5-4031-ac2b-d207dd229ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \".\"\n",
    "\n",
    "pytorch_state_dict_url = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/8J2QEyQqD8x9zjrlnv6N7g/ai-capstone-pytorch-best-model-20250713.pth\"\n",
    "pytorch_state_dict_name = \"ai_capstone_pytorch_best_model_state_dict_downloaded.pth\"\n",
    "pytorch_state_dict_path = os.path.join(data_dir, pytorch_state_dict_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5edf8ee-dab8-4a70-afd1-0a6cb65e7992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model file already downloaded at: ./ai_capstone_pytorch_best_model_state_dict_downloaded.pth\n"
     ]
    }
   ],
   "source": [
    "await download_model(pytorch_state_dict_url, pytorch_state_dict_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53c7de30-36e4-4dae-bd80-be7442bb4c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int = 42) -> None:\n",
    "    \"\"\"Seed Python, NumPy, and PyTorch (CPU & all GPUs) and\n",
    "    make cuDNN run in deterministic mode.\"\"\"\n",
    "    # ---- Python and NumPy -------------------------------------------\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    # ---- PyTorch (CPU  &  GPU) --------------------------------------\n",
    "    torch.manual_seed(seed)            \n",
    "    torch.cuda.manual_seed_all(seed)   \n",
    "\n",
    "    # ---- cuDNN: force repeatable convolutions -----------------------\n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark     = False \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff9c5ae1-9248-431f-a606-5778f2f5ca01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global seed set to 7331 - main process is now deterministic.\n"
     ]
    }
   ],
   "source": [
    "SEED = 7331\n",
    "set_seed(SEED)\n",
    "print(f\"Global seed set to {SEED} - main process is now deterministic.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a120e6b-5689-4be7-9709-d7ebfa3cd3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    ''' \n",
    "    Class to define the architecture same as the imported pre-trained CNN model for extracting the` feature map\n",
    "    '''\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(32),\n",
    "            nn.Conv2d(32, 64, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(64),\n",
    "            nn.Conv2d(64, 128, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(128),\n",
    "            nn.Conv2d(128, 256, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(256),\n",
    "            nn.Conv2d(256, 512, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(512),\n",
    "            nn.Conv2d(512, 1024, 5, padding=2), nn.ReLU(), nn.MaxPool2d(2), nn.BatchNorm2d(1024)\n",
    "        )\n",
    "\n",
    "    def forward_features(self, x):\n",
    "        return self.features(x)      # (B,1024,H,W)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6e6958-07b3-4037-9f91-d7ac6e4d2f81",
   "metadata": {},
   "source": [
    "# Vision Transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01a517d5-b6fa-4fe5-a848-02cfc84106eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbed(nn.Module):\n",
    "    def __init__(self, input_channel=1024, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.proj = nn.Conv2d(input_channel, embed_dim, kernel_size=1)  # 1×1 conv\n",
    "    def forward(self, x):\n",
    "        x = self.proj(x).flatten(2).transpose(1, 2)  # (B,L,D)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb7b99ee-9978-4e52-81d8-223342e8bf7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI HEAD SELF ATTENTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f7114dc-5a8b-4aea-8509-e2dd13bb8f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MHSA(nn.Module):\n",
    "    def __init__(self, dim, heads=8, dropout=0.):\n",
    "        super().__init__()\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5\n",
    "        self.qkv = nn.Linear(dim, dim * 3)\n",
    "        self.attn_drop = nn.Dropout(dropout)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(dropout)\n",
    "    def forward(self, x):\n",
    "        B, N, D = x.shape\n",
    "        q, k, v = self.qkv(x).chunk(3, dim=-1)\n",
    "        q = q.reshape(B, N, self.heads, -1).transpose(1, 2)  # (B, heads, N, d)\n",
    "        k = k.reshape(B, N, self.heads, -1).transpose(1, 2)\n",
    "        v = v.reshape(B, N, self.heads, -1).transpose(1, 2)\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "        attn = self.attn_drop(attn.softmax(dim=-1))\n",
    "        x = torch.matmul(attn, v).transpose(1, 2).reshape(B, N, D)\n",
    "        return self.proj_drop(self.proj(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64f158e5-3c37-4682-8fe8-1db3d6813dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_ratio=4., dropout=0.):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn  = MHSA(dim, heads, dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.mlp   = nn.Sequential(\n",
    "                                    nn.Linear(dim, int(dim * mlp_ratio)),\n",
    "                                    nn.GELU(), nn.Dropout(dropout),\n",
    "                                    nn.Linear(int(dim * mlp_ratio), dim),\n",
    "                                    nn.Dropout(dropout))\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x))\n",
    "        x = x + self.mlp(self.norm2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "820485d9-8bde-4edf-981c-41259456b334",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vision Transformer (ViT) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4b1fd0bb-d07d-4b32-b441-2757d8b931b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, in_ch=1024, num_classes=2,\n",
    "                 embed_dim=768, depth=6, heads=8,\n",
    "                 mlp_ratio=4., dropout=0.1, max_tokens=50):\n",
    "        super().__init__()\n",
    "        self.patch = PatchEmbed(in_ch, embed_dim)           # 1×1 conv\n",
    "        self.cls   = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos   = nn.Parameter(torch.randn(1, max_tokens, embed_dim))\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, heads, mlp_ratio, dropout)\n",
    "            for _ in range(depth)])\n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):                          # x: (B,C,H,W)\n",
    "        x = self.patch(x)                          # (B,L,D)\n",
    "        B, L, _ = x.shape\n",
    "        cls = self.cls.expand(B, -1, -1)           # (B,1,D)\n",
    "        x = torch.cat((cls, x), 1)                 # (B,L+1,D)\n",
    "        x = x + self.pos[:, :L + 1]                # match seq-len\n",
    "        for blk in self.blocks:\n",
    "            x = blk(x)\n",
    "        return self.head(self.norm(x)[:, 0])       # CLS token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88e38da-ff2e-4cdc-a720-85f1b4ef2e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN-ViT hybrid model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbf7d20d-9a55-4398-90a9-81e860f62838",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_ViT_Hybrid(nn.Module):\n",
    "    def __init__(self, num_classes=2, embed_dim=768, depth=6, heads=8):\n",
    "        super().__init__()\n",
    "        self.cnn = ConvNet(num_classes)            # load weights later\n",
    "        self.vit = ViT(num_classes=num_classes,\n",
    "                       embed_dim=embed_dim,\n",
    "                       depth=depth,\n",
    "                       heads=heads)\n",
    "    def forward(self, x):\n",
    "        return self.vit(self.cnn.forward_features(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd0c2e6d-7d11-4de1-a566-a0c48cd5b8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL TRAINING AND EVALUATION FOR ONE EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "27c5540f-b131-4cfd-9d18-93d51b4dca23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    loss_sum, correct = 0, 0\n",
    "    for batch_idx, (x, y) in enumerate(tqdm(loader, desc=\"Training  \")):\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(x)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_sum += loss.item() * x.size(0)\n",
    "        correct  += (out.argmax(1) == y).sum().item()\n",
    "    return loss_sum / len(loader.dataset), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "eaa42f92-0538-4490-a62f-8d1d37b64fbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, loader, criterion, device):\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        loss_sum, correct = 0, 0\n",
    "        for batch_idx, (x, y) in enumerate(tqdm(loader, desc=\"Validation\")):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            loss = criterion(out, y)\n",
    "            loss_sum += loss.item() * x.size(0)\n",
    "            correct  += (out.argmax(1) == y).sum().item()\n",
    "    return loss_sum / len(loader.dataset), correct / len(loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c925c5c6-80f1-42f2-9212-f4c02bc50d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(data_dir, \"images_dataSAT\")\n",
    "\n",
    "img_size = 64\n",
    "batch_size = 32\n",
    "lr = 0.001\n",
    "num_cls  = 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b411ea27-22c9-404a-973b-e0c362e190f9",
   "metadata": {},
   "source": [
    "### Training data transformations\n",
    "The **training transform** pipeline implements several **augmentation techniques** including Random Rotation, Random Horizontal Flip, Random Affine with Shear and normalization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43b7aec3-13f2-4b33-8093-2edb676a86b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##====================================================================================\n",
    "#    creating a train_transform transform for the training dataset\n",
    "#====================================================================================\n",
    "train_transform = transforms.Compose([transforms.Resize((img_size, img_size)),\n",
    "                                      transforms.RandomRotation(40),\n",
    "                                      transforms.RandomHorizontalFlip(),\n",
    "                                      transforms.RandomAffine(0, shear=0.2),\n",
    "                                      transforms.ToTensor(),\n",
    "                                      transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n",
    "                                     ]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c55c42-34cb-40ba-821a-f1d3f870ae0d",
   "metadata": {},
   "source": [
    "### Validation data transformations\n",
    "The **validation transform** is minimal for **deterministic preprocessing** to ensure reproducible validation results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7f8d96f8-c895-4fc7-9818-384393dc238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#======================================================================================================\n",
    "#       creating a val transform for the validation dataset\n",
    "#======================================================================================================\n",
    "val_transform = transforms.Compose([transforms.Resize((img_size, img_size)), \n",
    "                                    transforms.ToTensor(),\n",
    "                                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "                                   ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e68dee90-ccc1-4933-b55e-cfa1ec1b2fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_dataset = datasets.ImageFolder(dataset_path, transform=train_transform)\n",
    "\n",
    "train_size = int(0.8 * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "train_dataset.dataset.transform = train_transform\n",
    "val_dataset.dataset.transform = val_transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1d72de3d-4f8e-445b-991d-9474ce10ffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## creating a dataloader train_loader and val_loader using train_dataset and val_dataset\n",
    "\n",
    "train_loader = DataLoader(train_dataset, \n",
    "                          batch_size = batch_size,\n",
    "                          shuffle = True,\n",
    "                         ) \n",
    "\n",
    "val_loader = DataLoader(val_dataset, \n",
    "                        batch_size=batch_size,\n",
    "                        shuffle = False,\n",
    "                       ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1605bf28-929b-4eda-8da7-61ef0f8b0f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on cpu\n",
      "epochs:5 | batch:32 | attn_heads:6 | depth:3 | embed_dim:768\n",
      "\n",
      "Epoch 01/05 started at 20251214_122854 (UTC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training  : 100%|██████████| 150/150 [12:27<00:00,  4.98s/it]\n",
      "Validation: 100%|██████████| 38/38 [00:54<00:00,  1.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train loss 0.3249 acc 0.9354 | val loss 0.0399 acc 0.9858 | in  801.20s\n",
      "Current loss (0.0399) lower than previous best loss (inf), Saving current model state\n",
      "\n",
      "Epoch 02/05 started at 20251214_124218 (UTC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training  :  59%|█████▉    | 89/150 [06:29<04:27,  4.38s/it]\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './images_dataSAT/class_1_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UPE_20250427T170513.SAFE_21150.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 41\u001b[0m\n\u001b[1;32m     39\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepochs\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m started at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpresent_time()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (UTC)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 41\u001b[0m tr_loss,tr_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m te_loss,te_acc \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader, criterion, device)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m02d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     44\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtr_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     45\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval loss \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mte_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m acc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mte_acc\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m |\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     46\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtime\u001b[38;5;241m.\u001b[39mtime()\u001b[38;5;241m-\u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.02f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     47\u001b[0m     )\n",
      "Cell \u001b[0;32mIn[16], line 4\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, optimizer, criterion, device)\u001b[0m\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      3\u001b[0m loss_sum, correct \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 4\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining  \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[1;32m   1184\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:734\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    732\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 734\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    735\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    737\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    738\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    740\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataloader.py:790\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    789\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    791\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    792\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:50\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_collation:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__getitems__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__:\n\u001b[0;32m---> 50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__getitems__\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torch/utils/data/dataset.py:416\u001b[0m, in \u001b[0;36mSubset.__getitems__\u001b[0;34m(self, indices)\u001b[0m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices])  \u001b[38;5;66;03m# type: ignore[attr-defined]\u001b[39;00m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 416\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m indices]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torchvision/datasets/folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torchvision/datasets/folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.12/site-packages/torchvision/datasets/folder.py:262\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpil_loader\u001b[39m(path: Union[\u001b[38;5;28mstr\u001b[39m, Path]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Image\u001b[38;5;241m.\u001b[39mImage:\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\u001b[39;00m\n\u001b[0;32m--> 262\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    263\u001b[0m         img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m img\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './images_dataSAT/class_1_agri/tile_S2A_MSIL2A_20250427T101701_N0511_R065_T32UPE_20250427T170513.SAFE_21150.jpg'"
     ]
    }
   ],
   "source": [
    "device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training the model on {device}\")\n",
    "\n",
    "\n",
    "epochs     = 5\n",
    "attn_heads = 6\n",
    "depth      = 3\n",
    "embed_dim  = 768\n",
    "\n",
    "print(f\"epochs:{epochs} | batch:{batch_size} | attn_heads:{attn_heads} | depth:{depth} | embed_dim:{embed_dim}\")\n",
    "\n",
    "model_dict_name = f\"ai_capstone_pytorch_vit_model_state_dict.pth\"\n",
    "\n",
    "model     = CNN_ViT_Hybrid(num_classes=num_cls,\n",
    "                            heads=attn_heads,\n",
    "                            depth=depth,\n",
    "                            embed_dim=embed_dim\n",
    "                           ).to(device)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------ #\n",
    "# loading pre-trained CNN weights\n",
    "model.cnn.load_state_dict(torch.load(pytorch_state_dict_path), strict=False)\n",
    "# ------------------------------------------------------------------ #\n",
    "\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "\n",
    "best_loss = float('inf')\n",
    "tr_loss_all = []\n",
    "te_loss_all = []\n",
    "tr_acc_all = []\n",
    "te_acc_all = []\n",
    "\n",
    "training_time = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nEpoch {epoch:02d}/{epochs:02d} started at {present_time()} (UTC)\")\n",
    "    tr_loss,tr_acc = train(model, train_loader, optimizer, criterion, device)\n",
    "    te_loss,te_acc = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
    "          f\"val loss {te_loss:.4f} acc {te_acc:.4f} |\"\n",
    "          f\" in  {time.time()-start_time:.02f}s\"\n",
    "        )\n",
    "    tr_loss_all.append(tr_loss)\n",
    "    te_loss_all.append(te_loss)\n",
    "    tr_acc_all.append(tr_acc)\n",
    "    te_acc_all.append(te_acc)\n",
    "    training_time.append(time.time() - start_time)\n",
    "    \n",
    "    # Save the best model\n",
    "    avg_te_loss = te_loss\n",
    "    if avg_te_loss < best_loss:\n",
    "        print(f\"Current loss ({avg_te_loss:.04f}) lower than previous best loss ({ best_loss:.04f}), Saving current model state\")\n",
    "        best_loss = avg_te_loss\n",
    "        torch.save(model.state_dict(), model_dict_name)\n",
    "\n",
    "print(f\"epochs:{epochs} | batch:{batch_size} | attn_heads:{attn_heads} | depth:{depth} | embed_dim:{embed_dim}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668583e1-9f69-4b2d-83dd-78d7ecbb5e27",
   "metadata": {},
   "source": [
    "### Hyperparameter cheatsheet (depth based)\n",
    "\n",
    "The Depth of the transformer blocks signify the number of transformer blocks stacked in the model. This is one of the most important features which differentiates a ViT from CNN.\n",
    "\n",
    "This table proides a basic overview on **how depth affects** other hyperparameters and model performance.\n",
    "\n",
    "|  **Depth** | **Attention heads** | **Dataset Size** |  **Performance** | **learning rate** | **Feature Complexity** | **Learning Focus** |\n",
    "|:---:|:---:|---|:---:|---|:---|:---:|\n",
    "| **3** | 6 | size < 1000 | Underfitting - too shallow |0.001 (Shallow: can handle higher learning rates) | Low-level features | Edges, textures, basic patterns |\n",
    "| **6** | 6 | size <1000 | Good for simple tasks |0.001 (Shallow: can handle higher learning rates) | Mid-level features | Shapes, object parts, spatial relationships |\n",
    "| **12** | 12 | 1000 < size < 10000 | Standard choice - good balance | 0.0005 (Medium: moderate learning rate) | High-level features | Objects, semantic concepts, global context |\n",
    "| **18** | 12 | 10000 < size < 100000 | High performance on complex tasks | 0.0003 (Deep: lower learning rate for stability) | High-level features | Objects, semantic concepts, global context |\n",
    "| **24** | 16 | 100000 < size | Diminishing returns, overfitting risk | 0.0001 (Very deep: very small learning rate) | High-level features | Objects, semantic concepts, global context |\n",
    "| **36** | 16 | 100000 < size | Likely overkill for most tasks | 0.0001 (Very deep: very small learning rate) | High-level features | Objects, semantic concepts, global context |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c542f0ad-6d11-4afc-b21d-ffa6a7bf8330",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the model on cpu\n",
      "epochs:5 | batch:32 | attn_heads:12 | depth:12 | embed_dim:768\n",
      "\n",
      "Epoch 01/05 started at 20251214_134247 (UTC)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training  :   0%|          | 0/150 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "device   = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Training the model on {device}\")\n",
    "\n",
    "epochs     = 5\n",
    "attn_heads = 12\n",
    "depth      = 12\n",
    "embed_dim  = 768\n",
    "\n",
    "print(f\"epochs:{epochs} | batch:{batch_size} | attn_heads:{attn_heads} | depth:{depth} | embed_dim:{embed_dim}\")\n",
    "\n",
    "model_dict_name = f\"ai_capstone_pytorch_vit_model_test_state_dict.pth\"\n",
    "\n",
    "model_test = CNN_ViT_Hybrid(num_classes=num_cls,\n",
    "                            heads=attn_heads,\n",
    "                            depth=depth,\n",
    "                            embed_dim=embed_dim\n",
    "                           ).to(device)\n",
    "#==============================================================================================================\n",
    "#     loading pre-trained CNN weights \n",
    "#==============================================================================================================\n",
    "\n",
    "\n",
    "\n",
    "criterion= nn.CrossEntropyLoss()\n",
    "optimizer= torch.optim.Adam(model_test.parameters(), lr=lr)\n",
    "\n",
    "best_loss = float('inf')\n",
    "tr_loss_all_test = []\n",
    "te_loss_all_test = []\n",
    "tr_acc_all_test = []\n",
    "te_acc_all_test = []\n",
    "training_time_test = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    start_time = time.time()\n",
    "    print(f\"\\nEpoch {epoch:02d}/{epochs:02d} started at {present_time()} (UTC)\")\n",
    "    tr_loss,tr_acc = train(model_test, train_loader, optimizer, criterion, device)\n",
    "    te_loss,te_acc = evaluate(model_test, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch:02d} | \"\n",
    "          f\"train loss {tr_loss:.4f} acc {tr_acc:.4f} | \"\n",
    "          f\"val loss {te_loss:.4f} acc {te_acc:.4f} |\"\n",
    "          f\" in  {time.time()-start_time:.02f}s\"\n",
    "        )\n",
    "    tr_loss_all_test.append(tr_loss)\n",
    "    te_loss_all_test.append(te_loss)\n",
    "    tr_acc_all_test.append(tr_acc)\n",
    "    te_acc_all_test.append(te_acc)\n",
    "    training_time_test.append(time.time() - start_time)\n",
    "\n",
    "    # Save the best model\n",
    "    avg_te_loss = te_loss\n",
    "    if avg_te_loss < best_loss:\n",
    "        print(f\"Current loss ({avg_te_loss:.04f}) lower than previous best loss ({ best_loss:.04f}), Saving current model state\")\n",
    "        best_loss = avg_te_loss\n",
    "        torch.save(model_test.state_dict(), model_dict_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "plot-doc",
   "metadata": {},
   "source": [
    "## Plotting training and validation accuracy and loss\n",
    "\n",
    "This visualization cell creates comprehensive learning curves that provide crucial insights into model training dynamics, performance trends, and potential issues such as overfitting or underfitting.\n",
    "\n",
    "The implementation creates two separate plots for different aspects of training analysis:\n",
    "- **Accuracy plot**: Shows classification performance trends over epochs\n",
    "- **Loss plot**: Reveals optimization dynamics and convergence behavior\n",
    "\n",
    "\n",
    "These plots enable several important diagnostic assessments:\n",
    "- **Overfitting detection**: Widening gap between training and validation metrics\n",
    "- **Underfitting identification**: Both metrics plateau at suboptimal levels\n",
    "- **Training completion**: Convergence indicates when to stop training\n",
    "- **Hyperparameter evaluation**: Curves help assess learning rate, regularization effectiveness\n",
    "\n",
    "These learning curves serve as essential tools for understanding model behavior, diagnosing training issues, and making informed decisions about hyperparameter adjustments, training duration, and model architecture modifications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0050164-56da-44b4-99f5-1851f142aa10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_w, fig_h = 6,4\n",
    "fig, axs = plt.subplots(figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Accuracy on the first subplot\n",
    "axs.plot(tr_acc_all, label='Training Accuracy')\n",
    "axs.plot(te_acc_all, label='Validation Accuracy')\n",
    "axs.set_title('Model Accuracy')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Accuracy')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots( figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Loss on the second subplot\n",
    "axs.plot(tr_loss_all, label='Training Loss')\n",
    "axs.plot(te_loss_all, label='Validation Loss')\n",
    "axs.set_title('Model Loss')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3158bb89-889d-430b-90c2-d54c4e313171",
   "metadata": {},
   "source": [
    "## Task: Compare the performance of `model` with `model_test` by plotting the validation loss for `model` and `model_test` ViTs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3cf54d-9029-4094-81ad-e89e6b16ee62",
   "metadata": {},
   "outputs": [],
   "source": [
    "## compare the performance of model with model_test by plotting the validation loss for model and model_test ViTs\n",
    "\n",
    "\n",
    "axs.plot(te_loss_all, label='Validation Loss (model)') \n",
    "axs.plot(te_loss_all_test, label='Validation Loss (model_test)') \n",
    "axs.set_title('Model Loss') \n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038c595d-12ed-4e30-8ec9-bb615ca3c591",
   "metadata": {},
   "source": [
    "Double-click **here** for the solution.\n",
    "<!--\n",
    "\n",
    "fig, axs = plt.subplots( figsize=(fig_w, fig_h ))\n",
    "\n",
    "# Plot Loss on the second subplot\n",
    "axs.plot(te_loss_all, label='Validation Loss (model)')\n",
    "axs.plot(te_loss_all_test, label='Validation Loss (model_test)')\n",
    "axs.set_title('Model Loss')\n",
    "axs.set_xlabel('Epochs')\n",
    "axs.set_ylabel('Loss')\n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c980db06-f25b-43f5-a20b-4f5e8363bf7c",
   "metadata": {},
   "source": [
    "## Task: Compare the training times of `model` with `model_test` by plotting the training time for each\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ef17b1-d965-4706-93d3-0b824fce7313",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Please use the space below to write your answer\n",
    "\n",
    "axs.plot(training_time, label='Training time (model)')\n",
    "axs.plot(training_time_test, label='Training time (model_test)')\n",
    "axs.set_title('Training time') \n",
    "axs.set_xlabel('Epochs') \n",
    "axs.set_ylabel('Seconds') \n",
    "axs.legend()\n",
    "axs.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e075dc2f-6ffa-45a6-b2d8-860217305244",
   "metadata": {},
   "source": [
    "<!--\n",
    "## Change Log\n",
    "\n",
    "|  Date (YYYY-MM-DD) |  Version | Changed By  |  Change Description |\n",
    "|---|---|---|---|\n",
    "| 2025-07-24  | 1.0  | Aman  |  Created the lab |\n",
    "| 2025-07-24  | 1.1  | Leah Hanson  | QA reviewed for IBM style guide adherence |\n",
    "\n",
    "-->\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "prev_pub_hash": "3ad35712b296d635757464e3c9bafd86292bd10e56c19d75c4d9a65d356701cc"
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
